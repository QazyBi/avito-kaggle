{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch processing \n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "IMG_PATH = '../data/train_images/'\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a GPU available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Define DINO v2 preprocessing (adjust based on the model's requirements)\n",
    "def dinov2_preprocess(image: Image.Image):\n",
    "    # Example preprocessing steps; adjust as needed\n",
    "    image_transforms = T.Compose([\n",
    "        T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    return image_transforms(image)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.image_paths = df[\"image\"].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(Path(IMG_PATH, f\"{img_path}.jpg\")).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading image {img_path}: {e}\")\n",
    "            # create PIL empty image\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        return self.transform(image), img_path\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, paths = zip(*batch)\n",
    "    return torch.stack(images), paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/qb/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /home/qb/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n",
      "100%|██████████| 330M/330M [00:11<00:00, 30.5MB/s] \n",
      "100%|██████████| 23491/23491 [3:08:46<00:00,  2.07it/s]  \n"
     ]
    }
   ],
   "source": [
    "# clip\n",
    "\n",
    "use_clip = False\n",
    "\n",
    "if use_clip:\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    dataset = ImageDataset(df, preprocess)\n",
    "else:\n",
    "    dinov2_model = torch.hub.load(\n",
    "        \"facebookresearch/dinov2\", \"dinov2_vitb14\", pretrained=True\n",
    "    )\n",
    "    dinov2_model.to(device)\n",
    "    dinov2_model.eval()\n",
    "    dataset = ImageDataset(df, dinov2_preprocess)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = {}\n",
    "iter_id = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        if use_clip:\n",
    "            image_features = model.encode_image(images)\n",
    "        else:\n",
    "            image_features = dinov2_model(images)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        image_features = image_features.cpu().numpy()\n",
    "        embeddings[paths] = image_features\n",
    "        iter_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23491"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dict size\n",
    "len(embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23491/23491 [00:03<00:00, 6816.84it/s] \n"
     ]
    }
   ],
   "source": [
    "# get flattened embeddings\n",
    "# currently each key contains list of file ids \n",
    "# and each value contains list of embeddings\n",
    "\n",
    "new_embeddings = {}\n",
    "for key, value in tqdm(embeddings.items()):\n",
    "    for k, v in zip(key, value):\n",
    "        new_embeddings[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503424"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../outputs/data/feat/dino_embeddings.npz', embeddings=list(new_embeddings.values()), images=list(new_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv features.npz ../outputs/data/feat/clip_embeddings.npz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
