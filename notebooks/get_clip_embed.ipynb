{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch processing \n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "IMG_PATH = '../data/train_images/'\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a GPU available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.image_paths = df[\"image\"].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(Path(IMG_PATH, f\"{img_path}.jpg\")).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading image {img_path}: {e}\")\n",
    "            # create PIL empty image\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        return self.transform(image), img_path\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, paths = zip(*batch)\n",
    "    return torch.stack(images), paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23491/23491 [15:46<00:00, 24.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageDataset(df, preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = {}\n",
    "iter_id = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features.cpu().numpy()\n",
    "        embeddings[paths] = image_features\n",
    "        iter_id += 1\n",
    "        # if iter_id > 10:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23491"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dict size\n",
    "len(embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23491/23491 [00:00<00:00, 63814.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# get flattened embeddings\n",
    "# currently each key contains list of file ids \n",
    "# and each value contains list of embeddings\n",
    "\n",
    "new_embeddings = {}\n",
    "for key, value in tqdm(embeddings.items()):\n",
    "    for k, v in zip(key, value):\n",
    "        new_embeddings[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503424"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('features.npz', embeddings=list(new_embeddings.values()), images=list(new_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv features.npz ../outputs/data/feat/clip_embeddings.npz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
