{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch processing \n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "IMG_PATH = '../data/train_images/'\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a GPU available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Define DINO v2 preprocessing (adjust based on the model's requirements)\n",
    "def dinov2_preprocess(image: Image.Image):\n",
    "    # Example preprocessing steps; adjust as needed\n",
    "    image_transforms = T.Compose([\n",
    "        T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    return image_transforms(image)\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.image_paths = df[\"image\"].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(Path(IMG_PATH, f\"{img_path}.jpg\")).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading image {img_path}: {e}\")\n",
    "            # create PIL empty image\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        return self.transform(image), img_path\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, paths = zip(*batch)\n",
    "    return torch.stack(images), paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/qb/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "  2%|‚ñè         | 429/23491 [03:07<2:48:22,  2.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m dinov2_model(images)\n\u001b[1;32m     32\u001b[0m image_features \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m/\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 34\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[43mimage_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     35\u001b[0m embeddings[paths] \u001b[38;5;241m=\u001b[39m image_features\n\u001b[1;32m     36\u001b[0m iter_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# clip\n",
    "\n",
    "use_clip = False\n",
    "\n",
    "if use_clip:\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    dataset = ImageDataset(df, preprocess)\n",
    "else:\n",
    "    dinov2_model = torch.hub.load(\n",
    "        \"facebookresearch/dinov2\", \"dinov2_vitb14\", pretrained=True\n",
    "    )\n",
    "    dinov2_model.to(device)\n",
    "    dinov2_model.eval()\n",
    "    dataset = ImageDataset(df, dinov2_preprocess)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = {}\n",
    "iter_id = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        if use_clip:\n",
    "            image_features = model.encode_image(images)\n",
    "        else:\n",
    "            image_features = dinov2_model(images)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        image_features = image_features.cpu().numpy()\n",
    "        embeddings[paths] = image_features\n",
    "        iter_id += 1\n",
    "\n",
    "\n",
    "# get flattened embeddings\n",
    "# currently each key contains list of file ids \n",
    "# and each value contains list of embeddings\n",
    "\n",
    "new_embeddings = {}\n",
    "for key, value in tqdm(embeddings.items()):\n",
    "    for k, v in zip(key, value):\n",
    "        new_embeddings[k] = v\n",
    "\n",
    "if use_clip:\n",
    "    filename = 'clip_embeddings.npz'\n",
    "else:\n",
    "    filename = 'dino_embeddings.npz'\n",
    "\n",
    "Path('../outputs/feat').mkdir(parents=True, exist_ok=True)\n",
    "np.savez_compressed(f'../outputs/feat/{filename}', embeddings=list(new_embeddings.values()), images=list(new_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,9G\t../outputs/feat/dino_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../outputs/feat/dino_embeddings.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f'../outputs/feat/{filename}', embeddings=list(new_embeddings.values()), images=list(new_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
